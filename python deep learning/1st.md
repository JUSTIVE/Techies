# Python Deep Learning

## Valentino Zocca, Gianmario Sapcagna, Daniel Slater, Peter Roelants

# Machine Learning - an Introduction

머신 러닝 기술은 많은 분야에 사용되어지고 있고, 데이터 과학자들은 다양한 산업에서 보이고 있다. 머신 러닝을 통해, 우리는 결정을 내리기 위하여 데이터에서 읽어내기 어려운 지식들을 얻는 프로세스를 식별합니다. 머신 러닝 기술이 사용될 수 있는 분야는 의학, 경제, 그리고 광고와 같이 매우 다양합니다.

이 챕터에서 우리는 다른 머신 러닝 접근법들과 기술들, 실제 세계의 문제에 대한 몇몇 적용예를 소개하고, `scikit-learn` 과 같은 파이썬으로 작성된 주요 오픈 소스 패키지를 소개합니다.
이 장에서는 머신 러닝이 무엇이고, 무엇을 할 수 있는지에 대한 요약과, 기존의 머신 러닝 기술과 딥 러닝이 얼마나 다른지 독자들에게 더 나은 이해를 위한 도움을 제공합니다.

소챕터

- What is Machine Learning?
- Different Machine learning approaches
- Steps involved in machine learning systems
- Brief Description of popular techniques / algorithms
- Applications in real-life
- A popular open source package

## What is Machine Learning?

머신 러닝은 주로 `빅 데이터`나 `AI(인공지능)` 이라는 단어와 주로 같이 언급됩니다. 하지만 그 둘과는 조금 다릅니다. 머신 러닝이 무엇이고, 왜 이것이 유용한지 이해하기 위해서는 빅 데이터가 무엇이고 어떻게 머신 러닝이 적용되는지 이해하는 것이 중요합니다.
`빅 데이터`는 카메라, 센서, 인터넷 사이트의 증가로 인해 증가된어진 결과인 많은 data set을 부르는 용어이다. 
명확하게, 사람은 혼자서 홀로그런 많은 데이터를 분석하고 이해할 수 없으며, 머신 러닝 기술은 그러한 매우 큰 data set을 이해하고 분석하는 데에 사용되어진다. 머신 러닝은 대규모 데이터 처리와 많은 변수와 특징들을 포함한 복잡한 데이터셋을 처리하는 도구로써 사용되어진다. 머신 러닝 기술의 강점 중의 하나는, 그 중 특히 딥 러닝에서는, 많은 data set에서 스스로의 분석력과 예측력을 강화 하여 사용할 수 있다는 것이다. 다른 말로는, 머신 러닝 기술은, 큰 데이터 세트에서 가장 잘 학습할 수 있다. 

반면에, 머신 러닝의 예측 능력은 인공지능에 매우 잘 적용되어질 수 있다. 머신 러닝은 인공지능 시스템의 '뇌'로 생각되어질 수 있다. `인공 지능`은 주위의 환경에 상호작용 할 수 있는 시스템으로 정의되어 질 수 있다. 머신 러닝은 뇌로써 기계가 센서를 통해 읽어온 데이터를 분석할 수 있게 하고, 적절한 정답을 구할 수 있도록 한다. 

## Different Machine Learning approaches

우리가 봐온 머신러닝이라는 용어는 매우 일반적인 용도로 사용되고, 많은 data set에서 패턴을 찾아내는 기술 혹은, 알려진 데이터로부터 새로운 데이터를 기반으로 새로운 데이터에 대한 예측을 만드는 능력을 의미한다.
이것은 매우 일반적이고 널리 알려진 정의이며, 많은 여러 기술들을 포괄하고 있다. 머신 러닝은 크기 두 분야로 나눌 수 있다. `Supervised Learning:지도 학습` 그리고 `Unsupervised Learning:비지도 학습` 그리고 나머지 한 분야인 `Reinforcement Learning:강화 학습` 분야도 종종 추가되곤 한다.

### Supervised Learning: 지도 학습

머신 러닝 알고리즘의 첫 분야의 이름은 *supervised learning* 이다. *supervised learning* algorithm 은 라벨링되어진 데이터를 이용하여 비슷한 라벨링되어지지 않은 데이터를 분류하는 분야이다. 라벨링되어지 데이터는 이미 분류된 것들이며, 라벨링되어지지 않은 데이터는 분류되지 않은 것들이다. 라벨은, 앞으로 우리가 보게 되겠지만, 이산적일 수도 있고, 연속적일 수도 있다.
지도 학습을 연상하는 하나의 방법은 우리가 데이터셋에 정의된 함수 *f* 를 만드는 것이라고 이미지화 하는 것이다. 우리의 data set은 
*feature:특징*으로 구성된 정보를 구성하고 있을 것이다. 머신 러닝 알고리즘은 값을 살펴보고 실수값에 대해서는 회귀를, 혹은 이산적인 분류 집합의 범위로 값을 사상할 것이다. 이 알고리즘은 
라벨링된 데이터에 가장 적합한 함수를 정의할 때까지 많은 예제(라벨링되어진 데이터)를 수행할 것이다. 이 함수는 다음과 같이 정의되어진다.
>*f: space of features → classes = (discrete values or real values)*

우리는 분류를 데이터 점들을 다른 집합으로 구분하는 과정이라 생각할 수 있다. 한번이라도 우리가 특징을 정의하였다면, 우리의 data set은 특징 공간 우의 점으로 생각되어 질 수 있고, 각각의 점들은 다른 예제들을 상징한다. 머신 러닝 알고리즘 작업은 고차원 평면을 그려 점들을 다른 특성으로 구별할 것이다.

이후의 챕터에서, 우리는 회귀 혹은 분류의 몇몇 문제들을 보게 될 것이다. 
>NMIST

### Unsupervised Learning : 비 지도학습

머신 러닝 알고리즘의 두 번째 분류는 `unsupervised learning : 비 지도 학습`이라 불린다. 우리는 데이터에 라벨링을 하지 않고, 알고리즘이 스스로 결론을 내릴 수 있도록 한다. unsupervised learning의 가장 간단한, 그리고 가장 많이 쓰이는 예로는 `클러스터링:군집화`이 있다. `클러스터링:군집화`는 데이터를 하위 항목으로 분류하는 기술이다.

`클러스터링:군집화`가 작동하기 위해서ㄷ는 각 클러스터 내의 각 요소들은 집합 내의 공통점이 높고, 다른 집합과의 공통점이 낮은 원칙을 준수해야 합니다. 군집화는 집합의 수와 상관 없이 작동할 것이고, 군집화를 구성하는 k-means과 같은 개념은 원 데이터의 k-subset과 같은 부분 집합을 구하여 집합 밖의 요소보다 가까운 원소들을 찾는 것이다. 물론, 이러한 작업을 하기 위해서는 우리는 무엇이 *가까운* 것인지, 혹은 *유사한*것인지에 대해 정의해야 한다. 이것은, 우리가 정점간의 거리를 정의하는 단위를 정의해야 한다는 것이다.

`클러스터링:군집화`는 `unsupervised learning:비 지도 학습`만의 기술이 아니며, 우리는 딥 러닝의 최근 결과들이 `unsupervised learning:비 지도 학습` 작업에 매우 효과적이라는 것을 볼 것이다.

`unsupervised learning:비 지도 학습`의 장점 중 하나는 라벨링된 데이터가 필요 없다는 것이다. *Restricted Boltzman machine*과 같은 비 지도 딥 러닝 기술은 데이터의 익명의 특징들을 이용하여 작동한다.

### Reinforcement Learning : 강화 학습

머신러닝 기술의 세 번째 분류는 `reinforcement learning:강화학습`이라 불린다. 이것은 피드백 요소를 이용하여 스스로의 성능을 향상시키지만 `supervied learning:지도 학습`과는 다르게 동작한다. 강화학습 기술의의 일반적인 적용예는 기계에게 게임을 플레이하는 것을 학습시키는 것이다. 이 경우, 우리는 각각의 게임 내의 동작들에 대해 좋거나 나쁨을 라벨링하지 않지만, 득점이나 실점과 같은 게임 중의, 혹은 게임의 결과물을 이용한다. 강화학습은 이전의 게임의 승리와 같은 성공적인 행동을 이용하려는 경향이 있다. 하지만, 미지의 영역에서는, 알고리즘은 결과에 따라 게임의 구조를 더 깊이 배울 수 있는 새로운 시도해야만 한다. 동작들은 서로 상관이 있기 때문에, 하나하나의 동작들에 대해서 `좋음` 혹은 `나쁨` 등의 평가를 할 수 없는 대신, 전체의 동적 행동들을 묶어 평가한다. 이는 체스에서 가끔 폰을 희생하는 것과 같이 실점으로 평가되어지는, 일반적으로는 부정적인 결과임에도 불구하고, 행동이 만약 체스판에서의 좋은 자리배치를 가져온다면, 강화학습에서는 이러한 전체적인 문제를 학습한다.

## Steps Involved in machin learning systems

머신 러닝을 적용하기 위해서 우리가 중요하게 봐야 할 부분은 다음과 같이 설명되어질 수 있다.

- `Learner : 학습자` : 학습자는 알고리즘과 그것의 "학습 철학"이 사용되어지는 것을 의미한다. 다음 장에서 보게 되듯이, 많은 학습 문제에 적용될 수 있는 머신 러닝 기술들이 많다. 학습자의 선택은 다른 문제에 특정한 머신 러닝 알고리즘이 적합할 수 있기 때문에 중요하다.
- `Training Data : 학습 데이터` : 이것은 우리가 관심있는 원 데이터 셋이다. 데이터는 `supervised learning : 지도학습`을 위해 라벨링 되어있을 수도 있고, `unsupervised learning : 비 지도 학습`을 위해서라면 라벨링되어있지 않을 수도 있다. 학습자가 문제의 구조를 충분히 이해할 수 있도록 충분한 샘플 데이터를 확보하는 것이 중요하다.
- `Representation : 대표` : 이것은 학습자에게 제공되어질수 있도록 데이터가 선택된 특징으로 표현되는 방식이다. 대표의 좋은 선택은 좋은 결과를 달성하기 위해서는 매우 중요하다.
- `Goal : 목표` : 이것은 손에서 문제를 위해 데이터를 학습할 이유를 의미한다. 이것은 대상에 강하게 관련이 있어야 하며, 어떤 학습자가 사용되어질지, 그리고 어떤 대표를 통해 사용해야할지 정의하는 것을 돕는다.
- `Target : 대상` : 이것은 최종 결과물과 무엇이 학습되어지고 있는지를 의미한다. 이것은 라벨링되어지지 않은 데이터에 대한 분류일 수도 있고, 입력 데이터에 따른 숨겨진 패턴이나 특징의 표현일 수도 있다. 

머신 러닝 알고리즘은 문제에 대한 정확한 수치적인 해결법이 아닌, 그냥 예측이다.

일반적으로, 고전적인 머신 러닝으로 풀 수 있는 문제들을 위한 기술들은 배포 이전에 학습 데이터에 대한 전반적인 이해와 가공이 필요하겠지만, 만약 머신 러닝 문제에 접근할 때의 몇 단계들을 정리하자면 다음과 같이 나타낼 수 있다.

- `Data Collection : 데이터 수집` : 이것은 가능한 한 많은 데이터를 수집하고, 지도 학습의 문제의 경우에는 정확한 라벨링을 수행하는 것을 의미한다.
- `Data Processing : 데이터 가공` : 이것은 데이터를 정리(예를 들어 의미 없는 데이터나 너무 상관관계가 높은 데이터를 제거하거나, 부족한 데이터를 채우는 것)하고, 학습 데이터를 정의하는 특징을 이해하는 것을 의미한다.
- `Creation of the test case : 테스트 케이스 생성` : 일반적으로 데이터 셋은 둘, 혹은 세개로 나뉜다.
  - 학습 데이터 셋
  - 검증 데이터 셋
  - 최종 테스트 셋

테스트 셋과 검증 셋을 만드는 검증된 이유가 있다. 우리가 언급한 듯이, 머신 러닝 기술들은 바라는 결과에의 예측만 만들어 낼 수 있다. 이것은 우리가 유한하고 제한적인 변수들을 포함할수 있고, 우리의 제어 밖의 많은 변수들이 존재 할 수 있다는 사실에 근거한다. 만약 우리가 단 하나의 데이터셋을 사용했고, 우리의 모델이 모든 데이터를 데이터를 전부 다 `기억`했다면 매우 높은 정확도로 기억한 데이터에 기반한 결과를 만들어 낼 수 있겠지만, 유사한 다른 데이터 셋에 대해서는 이러한 결과를 볼 수 없을 수도 있다. 머신 러닝 기술의 바람직한 목표 중 하나는 일반화할 수 있는 능력이다. 이것은 우리가 테스트 데이터셋을 만들어 학습이후에 우리의 모델을 `조율`하고, 최종 검증 데이터를 통하여 선택한 알고리즘의 정확도를 확인하는 이유이다.

데이터에서 유효한 특징을 선택하는 것과 오버피팅(데이터를 `기억`하는 것을 피하는 것)의 중요성을 이해하기 위해서는 [](http://xkcd/1122)를 확인해보자.

작은 특징들을 배운 머신 러닝 알고리즘은 큰 그림을 보기 힘들며, 결과는 만족스럽지 않을 것이다. 다른 말로, 오버피팅은 머신러닝이 숲을 잊어버리고 나무를 보게 하는 절차이다.
이것이 우리가 테스트 데이터와 트레이닝 데이터를 분리하는 이유이다: 만약 테스트 데이터의 정확도가 트레이닝 데이터에서의 정확도가 비슷하지 않다면, 이것은 우리의 모델이 오버피팅했다는 좋은 지표가 된다. 물론, 우리는 반대의 실수도 해서는 안된다. 이것은 언더 피팅이라 불린다. 실제로, 만약 우리가 우리의 예측 모델의 정확도를 트레이닝 데이터로 목표로 한다면, 언더피팅은 오버피팅만큼 리스크가 않고, 주요 관심사는 모델을 오버피팅하지 않게끔 하는 것이다.

## Brief desciption of popular techniques/algorithms

이 책의 서두에서 알고리즘을 그들의 `학습 스타일`로 지도학습/비지도학습/강화학습과 같이 나누는 것 대신, 우리는 그들의 구현체를 바탕으로 묶을 수 있다. 확실하게, 앞에서 언급한 각 집합들은 다른 머신 러닝 알고리즘을 통해 구현 되어질 수 있다. 

이 장에서는 다음을 소개할 것이다.
- `regression algorithm: 회귀 알고리즘`
- `linear regression: 선형 회귀`
- `고전 분류기`
  - `decision tres`
  - `naive Bayes`
  - `support vector machine`
- `unsupervised clustering algorithm`
  - `k-means`
- `reinforcement learning algorithm`
  -`cross-entropy`
- `neural networks`

### Linear Regression : 선형 회귀

회귀 알고리즘은 입력 데이터의 특징을 이용하여 값을 예측하는 지도 학습의 한 종류이다. 회귀 분석은 데이터 셋에서 가장 적합한 인자 내의 값을 찾으려 시도한다. 선형 회귀 알고리즘의 목표는, 목표값에 가장 가까운 함수의 적절한 인자를 입력된 데이터에서 찾아 `cost function`을 최소화하는 것이다. `cost function`은 정확한 결과에서 우리가 얼마나 떨어져 있는지를 나타내는 값인 `에러`에 대한 함수이다. 일반적인 `cost function`은 `Mean squared error: 평균 제곱 오차`이며, 이것은 기대값과 예측 결과의 차이의 제곱으로 계산한다. 모든 입력 예제의 합은 알고리즘의 오차를 나타내며, 이는 `cost function`을 의미합니다. 
> $$MSE = \frac{1}{n}\displaystyle\sum_{i=1}^{n}(Y_i- \hat{Y}_i) $$
[more information of MSE](https://en.wikipedia.org/wiki/Mean_squared_error)  
우리의 목표는 에러를 최대한 줄이는 것이기 때문에, 우리는 목표값 w에 대한 cost function의 미분 δ를 이용한다. 이 미분값은 함수가 증가하는 추세인지, 혹은 감소하는 추세인지의 방향을 나타내며, 움직이는 w가 미분의 반대 방향일 경우, 우리의 함수의 정확도를 증가시킬 것이다. cost function-에러의 최소값으로 향하도록 움직이는 것은 선형 회귀의 핵심이다. 물론, 우리는 우리는 미분의 방향으로 얼마나 빨리 움직일 지를 정해야 한다. cost function은 선형이 아니며, 우리는 미분값의 방향으로 아주 작은 단계씩 움직여야 한다. 너무 큰 단계는 최소값을 지나칠 수 있으며, 이는 수렴할 수 없다. 이러한 단계의 크기는 `learning rate`라고 부르며, 기호`lr`로 나타내어질 수 있다. $w=w-δ*lr$

위의 단계를 여러 번 반복하면 우리는 함수 *f*에 대한 최고의 결과*w*를 얻을 수 있을 것이다. 그러나 이 프로세스는 국지적으로만 작동할 것이며, 공간이 convex하지 않은 경우엔 전역적인 최고의 값을 찾지 못할 수도 있다는 것을 강조해야 한다. 만약 많은 국지적인 최소값이 존재하는 경우, 우리의 알고리즘은 저러한 국지적 최소값에 갇혀버릴 수 있으며, 전역적인 최소값까지 도달하지 못할 수 있다.

### Decision trees

지도학습에서 널리 사용되어지는 다른 방법은 `decision tree:결정 트리`이다. 결정 트리 알고리즘은 분류기를 하나의 트리의 형태로 만든다. 결정트리는 특정 속성에 대한 테스트를 수행하는 결정 노드로 구성되며, 말단 노드는 목표 속성의 값을 나타낸다. 결정 트리는 루트 노드에서 시작하여 말단 노드까지 내려오면서 작동하는 분류기의 종류이다.

결정트리는 다음과 같이 설명되어질 수 있다. 왼쪽 가지는 결정 노드의 긍정적인 대답이고, 오른쪽 가지는 결정 노드의 부정적인 대답이다. 각 가지의 끝은 말단 노드이다.

### K-means

우리가 이미 언급했던 클러스터링 알고리즘은, 비지도 학습의 방법 중 하나이다. 클러스터링의 대표적인 기술은 k-means 클러스터링이고, 이것은 모든 데이터셋의 모든 요소들을 `k distinct subset`이라 불리는 단위로 묶는 것이다. K-means는 상대적으로 간단한 과정이며, k개의 부분 집합의 중심을 나타내는 임의의 k점,혹은 `centroids`을 선택하는 것으로 시작한다. 그리고 각 centroid 마다 가까운 정점들을 선택한다. 이러한 과정은 `k-different subset`을 만들 것이다. 이 시점에서 각 부분집합마다 우리는 중심점을 다시 계산할 것이다. 그리고 우리는 위의 과정을 `centroids`가 움직이지 않을 때까지 계속 반복할 것이다.

이 기술이 작동하기 위해서는, 우리는 두 점간의 거리를 정의할 명확한 수치를 정의해야 한다. 이러한 절차는 다음과 같이 요약될 수 있다.

1. 초기 k-points를 선택한다.
2. 데이터의 각 점에 대해서 가장 가까운 `centroid`와 연관짓는다.
3. 특정 `centroid`에 연관된 점들의 새로운 중점을 계산한다.
4. 새로운 `centroid`의 새로운 중심을 정의한다.
5. 3과 4의 과정을 `centroid`가 움직이지 않을 때까지 반복한다.

이 방법이 초기의 임의의 centroid들에 민감하며, 다른 초기값으로 여러번 반복하는 것이 좋은 방법일 수도 있다. 또한, 몇 `centroids`들이 데이터셋의 어떠한 점과도 가깝지 않아 k의 수를 줄이는 것이 가능합니다. 우리는 같은 데이터셋에 대해서 결정트리와 k-means 클러스터링의 결과가 다를 수도 있습니다. 이것은 우리가 각 문제마다 적절한 머신 러닝 알고리즘을 고르는 것이 중요함을 상기합니다.

### Naïve Bayes

`Naïve Bayes`는 다른 머신 러닝 알고리즘과는 많이 다르다. 확률적으로, 머신 러닝 기술들이 하려는 것들은 조건 X에 대한 사건 Y의 확률을 평가하는 것이고, 이는 $p(X|Y)$ 로 표현된다. 

가끔은 우리는 반대의 정보를 가진다. 우리가 사건 $Y$ 에 대해 알고, 우리의 ㅎ